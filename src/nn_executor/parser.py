from logging import warning
from typing import List, Tuple, Union
import torch
from torch import nn
from nn_executor import models


def backward_connections(dst_node_idx:int,
                         node_inputs:Tuple[torch.Tensor,...],
                         saved_outputs:List[Tuple[torch.Tensor,int,int]],
                        ):
    connections = []
    # for each input
    for input_idx, input_t in enumerate(node_inputs):
        # find src of previously saved output tensors
        for t,src_node_idx, output_idx in saved_outputs[::-1]:
            if input_t.data_ptr() == t.data_ptr():
                connections.append((src_node_idx,output_idx, 
                                    dst_node_idx,input_idx))
                break
    
    return connections


def save_output_tensors(src_node_idx:int,
                        node_outputs:Tuple[torch.Tensor,...],
                        saved_outputs:List[Tuple[torch.Tensor,int,int]],
                        ):
    # for each output generated by node
    for output_idx, t in enumerate(node_outputs):
        # prevent saving the same tensor == save first occurence
        if t.data_ptr() not in [tt[0].data_ptr() for tt in saved_outputs]:
            saved_outputs.append((t,src_node_idx,output_idx))



SUPPORTED_MODULES = [
                    nn.Conv2d,
                    nn.MaxPool2d,
                    nn.BatchNorm2d,
                    nn.ReLU,
                    nn.LeakyReLU,
                    nn.Upsample,
                    models.Constant,
                    models.Variable,
                    models.Cat,
                    models.Sub,
                    models.Add,
                    models.Mul,
                    models.Identity,
                    models.Pruner,
                    models.ResidualPruner,
                    ]


class Parser:

    def __init__(self, supported_modules=SUPPORTED_MODULES) -> None:
        self.supported_modules = supported_modules
        self.layers:List[nn.Module] = []
        self.layers_indeces:List[int] = []
        self.layers_map:List[nn.Module] = []
        self.out_tensors:List[Tuple[torch.Tensor,int,int]] = [] # (t,src,out_idx)
        self.connections:List[Tuple[int,int,int,int]] = [] # (src,src_out_idx,dst,dst_inp_idx)
        
    def __call__(self, mod:nn.Module, 
                 input_tensors:Tuple[torch.Tensor,...], 
                 output_tensors:Tuple[torch.Tensor,...]):
        if mod.__class__ not in self.supported_modules:
            warning(mod.__class__.__name__+" is unsuported!\nThis could impact on correctness of parsing.")
            return
    
        self.layers.append(mod)
        node_idx = len(self.layers)
    
        # find source nodes        
        new_connections = backward_connections(dst_node_idx=node_idx,
                                               node_inputs=input_tensors,
                                               saved_outputs=self.out_tensors
                                               )
        # save new connections
        self.connections.extend(new_connections)
        
        save_output_tensors(src_node_idx=node_idx,
                            node_outputs=output_tensors,
                            saved_outputs=self.out_tensors
                            )
        
    def parse_module(self, module:nn.Module, *inputs:torch.Tensor):
        hook = lambda *x: self(*x)
        
        # set hooks for all modules without main
        for n, m in module.named_children():
            for mm in m.modules():
                mm.register_forward_hook(hook)
        
        self.add_inputs(*inputs)
        outs = module(*inputs)
        
        # TODO -> replace with only finding of src layers - do not create identity
        # connect outputs with their sources
        out_layer = models.Identity()
        self.__call__(out_layer, outs, ())
        
        self.layers_map = []
        self.layers_indeces = []
        
        for layer_node_idx, L in enumerate(self.layers):
            if L not in self.layers_map:
                self.layers_map.append(L)
            
            for idx, unique_layer in enumerate(self.layers_map):
                if id(L) == id(unique_layer):
                    self.layers_indeces.append(idx)
                    break
        
        return 
        
    def add_inputs(self, *args):
        # for zero node
        for i,t in enumerate(args):
            self.out_tensors.append((t,0,i))
            
    def __repr__(self) -> str:
        s = "Layers:\n"
        for L in self.layers:
            s += str(id(L)) + '\t' + str(L)+'\n'
            
        s += "Unique layers:\n"
        for L in self.layers_map:
            s += str(id(L)) + '\t' + str(L)+'\n'
            
        s += "Layers indeces:\n"
        s += str(self.layers_indeces)+'\n'
            
        s += 'Connections [(src,src_out_idx,dst,dst_in_idx)]:\n'
        for c in self.connections:
            s += str(c)+'\n'
        s += 'Outs [(id,shape,src,src_out_idx)]:\n'
        for o in self.out_tensors:
            s += str(id(o[0]))+', '
            s += str(tuple(o[0].shape))+', '
            s += str(o[1])+', '
            s += str(o[2])+'\n'
        
        return s
